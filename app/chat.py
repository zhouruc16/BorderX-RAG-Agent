from ollama import chat
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from utils import retrieve_context, load_local_index


def deepseek_chat(vectorstore: FAISS, query: str, k: int = 3, model: str = "deepseek-r1:1.5b") -> dict:
    """
    Processes a user query by retrieving context from the vector store,
    constructing a prompt, and sending it to the DeepSeek model using the
    ollama-python SDK. The response is parsed into an answer and references.
    
    Args:
        vectorstore (FAISS): The FAISS vector store containing document chunks.
        query (str): The user's query.
        k (int, optional): The number of top matching chunks to retrieve for context.
            Defaults to 3.
        model (str, optional): The DeepSeek model identifier to use.
            Defaults to "deepseek-r1:1.5b".
    
    Returns:
        dict: A dictionary with keys:
              "answer" - the answer text generated by DeepSeek.
              "references" - the list of source file names and page numbers referenced.
    """
    # Retrieve context from the FAISS vector store
    context = retrieve_context(vectorstore, query, k=k)
    
    # Construct the system and user messages as required by DeepSeek
    system_message = (
        "You are a helpful assistant. Answer the query only using the provided context. "
        "Do not include any information that is not present in the context. "
        "Include references to the source (file name and page number) in your answer."
    )
    
    user_message = (
        f"Context:\n{context}\n\n"
        f"Query: {query}\n\n"
        "Please respond using the following format exactly:\n"
        "Answer: <your answer here>\n"
        "References: <list of source file names and page numbers, separated by commas>"
    )
    
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]
    
    # Call the DeepSeek model via the ollama-python SDK
    response = chat(model=model, messages=messages)
    
    # Parse the response which is expected to be in the format:
    # Answer: <text>
    # References: <text>
    answer_lines = []
    references_lines = []
    current_section = None
    for line in response.message.content.splitlines():
        if line.startswith("Answer:"):
            current_section = "answer"
            answer_lines.append(line[len("Answer:"):].strip())
        elif line.startswith("References:"):
            current_section = "references"
            references_lines.append(line[len("References:"):].strip())
        else:
            if current_section == "answer":
                answer_lines.append(line.strip())
            elif current_section == "references":
                references_lines.append(line.strip())
    
    answer_text = "\n".join(answer_lines).strip()
    references_text = "\n".join(references_lines).strip()
    
    return {"answer": answer_text, "references": references_text}


def interactive_session(vectorstore: FAISS, model: str = "deepseek-r1:1.5b", k: int = 3) -> None:
    """
    Runs an interactive command-line session for user queries. For each query,
    relevant context is retrieved from the vector store and a prompt is constructed
    and sent to the DeepSeek model via the ollama-python SDK. The response is then displayed.
    
    Args:
        vectorstore (FAISS): The FAISS vector store containing document chunks.
        model (str, optional): The DeepSeek model identifier. Defaults to "deepseek-r1:1.5b".
        k (int, optional): The number of top matching chunks to retrieve for context.
            Defaults to 3.
    """
    print("Interactive DeepSeek session started. Type 'exit' or 'quit' to end.")
    while True:
        user_query = input("Enter your query: ").strip()
        if user_query.lower() in ("exit", "quit"):
            print("Exiting interactive session.")
            break
        
        result = deepseek_chat(vectorstore, user_query, k=k, model=model)
        print("\nResponse:")
        print(result["answer"])
        print("References:", result["references"])
        print("-" * 50)


def main() -> None:
    """
    Main function that loads the local FAISS vector store from disk,
    then starts an interactive session with the DeepSeek model via the ollama-python SDK.
    
    The user can enter queries which are answered using context retrieved from the FAISS index.
    """
    # Load the FAISS vector store from the local index directory
    vectorstore = load_local_index("index")
    print("Loaded local FAISS vector store from 'index'.")
    
    # Start the interactive query session
    interactive_session(vectorstore)
